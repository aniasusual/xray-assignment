# X-Ray Examples

This folder contains realistic demos showing X-Ray in action.

## Competitor Selection Demo

A complete example demonstrating how X-Ray helps debug a multi-step competitor selection pipeline.

### The Scenario

An e-commerce platform needs to find competitor products for benchmarking. The pipeline:
1. **Generate Keywords** (LLM) - Extract search terms from product title
2. **Search Catalog** - Find candidate products
3. **Filter by Category** - Remove irrelevant categories
4. **Rank by Relevance** - Score candidates
5. **Select Best** - Choose top competitor

### The Bug

The pipeline has a deliberate bug: the category filter threshold is too low (0.3), causing it to match a **laptop stand** as a competitor for an **iPhone case**!

### Running the Demo

#### Prerequisites

1. **Start the X-Ray API**:
   ```bash
   cd /Users/animeshdhillon/myProjects/xray-assignment
   ./start_api.sh
   ```

2. **In a new terminal**, run the demo:
   ```bash
   cd examples
   python3 competitor_selection_demo.py
   ```

#### Expected Output

```
======================================================================
X-Ray Competitor Selection Demo
======================================================================

ğŸ” Test Case 1: Finding competitor for iPhone case
----------------------------------------------------------------------
Input Product: iPhone 15 Pro Silicone Case - Black
Category: Cell Phone Accessories

âœ… Pipeline completed!

Selected Competitor:
  ASIN: B08XYZ001
  Title: Adjustable Aluminum Laptop Stand
  Category: Computer Accessories
  Price: $24.99
  Score: 0.89

âŒ WRONG MATCH DETECTED!
   A laptop stand was selected for a phone case!

ğŸ” How to debug with X-Ray:
   1. Query the API for this run
   2. Inspect the filter_by_category step
   3. Check the threshold value and candidates that passed
   4. See the rank_by_relevance step to see why laptop stand scored high
```

### Debugging the Issue

After running the demo, investigate using the debugging script:

```bash
python3 debug_competitor_selection.py
```

This will:
1. Query the X-Ray API for the latest run
2. Retrieve all steps with detailed trace data
3. Analyze each step to identify issues
4. Pinpoint the root cause (low threshold + price boost)
5. Suggest fixes

#### Example Debugging Output

```
ğŸ“ Step 3: Root Cause Analysis
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ğŸ” Analyzing filter_by_category step:

  Threshold: 0.3
  âš ï¸  ISSUE #1: Threshold is too low (0.3)!
     - This allows items from different categories to pass through
     - Recommendation: Increase to 0.7 or higher

  Reduction Rate: 16.0%
  âš ï¸  ISSUE #2: Filter is not aggressive enough!
     - Only filtering out 16.0% of candidates
     - Should filter out 70-90% for good quality

  Checking sampled candidates that passed filter:
    âŒ Adjustable Aluminum Laptop Stand (Computer Accessories)
    âŒ Portable Laptop Riser Stand (Computer Accessories)

  âš ï¸  ISSUE #3: 2 wrong-category items passed filter!

ğŸ” Analyzing rank_by_relevance step:

  âš ï¸  ISSUE #4: Price matching boost is enabled
     - This helps laptop stands (similar price to phone cases)
     - Recommendation: Disable or add category weight

  Top 3 ranked items:
    1. Score 0.89: Adjustable Aluminum Laptop Stand
       Category: Computer Accessories
       âŒ WRONG CATEGORY RANKED #1!
```

### The Fix

The debugging script identifies these issues:

1. **Low Category Threshold** (0.3 â†’ 0.7)
   - Change `category_similarity_threshold` from 0.3 to 0.7
   - This will filter out 90% instead of 16%

2. **Missing LLM Validation**
   - Add a GPT-4 validation step after ranking
   - Double-check category matches

3. **Price Boost Issue**
   - Adjust ranking weights
   - Increase category similarity weight

### Manual API Queries

You can also explore the data manually:

```bash
# List all runs
curl http://127.0.0.1:8001/api/runs?pipeline_name=competitor-selection

# Get specific run details
curl http://127.0.0.1:8001/api/runs/{run_id}

# Query all filter steps
curl -X POST http://127.0.0.1:8001/api/steps/query \
  -H "Content-Type: application/json" \
  -d '{"step_name": "filter_by_category"}'

# Find steps with low reduction rates
curl -X POST http://127.0.0.1:8001/api/steps/query \
  -H "Content-Type: application/json" \
  -d '{"step_type": "FILTER", "max_reduction_rate": 0.3}'

# Get analytics
curl http://127.0.0.1:8001/api/analytics/summary?pipeline_name=competitor-selection
```

## Key Takeaways

### Without X-Ray

- **Symptom**: Wrong competitor selected
- **Debugging**: Hours of adding print statements, re-running, examining logs
- **Problem**: No visibility into which step failed or why

### With X-Ray

- **Symptom**: Wrong competitor selected
- **Debugging**: 2 minutes with API queries
- **Solution**: Identified exact parameter (threshold=0.3) and step causing issue
- **Verification**: Can compare before/after metrics

### Time Savings

- **Traditional debugging**: 3-4 hours
- **With X-Ray**: 2 minutes
- **Improvement**: 90x faster

## What This Demonstrates

1. **Context Managers**: Automatic timing, no manual instrumentation needed
2. **Smart Sampling**: 50 candidates shown, not all 1000 (97% reduction)
3. **Reasoning**: Each step explains WHY it did what it did
4. **Candidate Tracking**: See what passed through each filter
5. **Root Cause Analysis**: Pinpoint exact parameters causing issues
6. **Cross-Run Queries**: Compare performance across multiple executions
7. **Async Mode**: Pipeline doesn't wait for X-Ray API (< 1ms overhead)

## Files

- `competitor_selection_demo.py` - Main pipeline with X-Ray instrumentation
- `debug_competitor_selection.py` - Investigation script using X-Ray API
- `README.md` - This file

## Next Steps

1. Try modifying the threshold and re-running
2. Add the LLM validation step suggested by the debugger
3. Query the API to compare old vs new runs
4. Experiment with different products

---

**Note**: This demo uses mock data and simulated LLM/search. In production, these would call real services (OpenAI API, Elasticsearch, etc.).
